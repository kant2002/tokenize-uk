import unittest
from tokenize_uk import UkrainianWordTokenizer


class TokenizeUkTest(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        cls.tokenizer = UkrainianWordTokenizer()

    def test_tokenize_url(self):
        url: str = "http://youtube.com:80/herewego?start=11&quality=high%3F"
        self.assertEqual([url, " "], self.tokenizer.tokenize(url + " "))

        url = "http://example.org"
        self.assertEqual([" ", url], self.tokenizer.tokenize(" " + url))

        url = "www.example.org"
        self.assertEqual([url], self.tokenizer.tokenize(url))

        url = "elect@ombudsman.gov.ua"
        self.assertEqual([url], self.tokenizer.tokenize(url))

        parts: list[str] = ["https://www.foo.com/foo", " ", "https://youtube.com", " ", "–ó–µ"]
        self.assertEqual(parts, self.tokenizer.tokenize("".join(parts)))

        parts = ["https://www.phpbb.com/downloads/", '"', ">", "—Å—Ç–æ—Ä—ñ–Ω–∫—É"]
        self.assertEqual(parts, self.tokenizer.tokenize("".join(parts)))

    def test_tokenize_tags(self):
        self.assertEqual(["<sup>", "3", "</sup>"], self.tokenizer.tokenize("<sup>3</sup>"))

    def test_numbers(self):
        test_list = self.tokenizer.tokenize("300 –≥—Ä–Ω –Ω–∞ –±–∞–ª–∞–Ω—Å—ñ")
        self.assertEqual(["300", " ", "–≥—Ä–Ω", " ", "–Ω–∞", " ", "–±–∞–ª–∞–Ω—Å—ñ"], test_list)

        test_list = self.tokenizer.tokenize("–Ω–∞–¥—ñ–π—à–ª–æ 2,2 –º—ñ–ª—å–π–æ–Ω–∞")
        self.assertEqual(["–Ω–∞–¥—ñ–π—à–ª–æ", " ", "2,2", " ", "–º—ñ–ª—å–π–æ–Ω–∞"], test_list)

        test_list = self.tokenizer.tokenize("–Ω–∞–¥—ñ–π—à–ª–æ 84,46 –º—ñ–ª—å–π–æ–Ω–∞")
        self.assertEqual(["–Ω–∞–¥—ñ–π—à–ª–æ", " ", "84,46", " ", "–º—ñ–ª—å–π–æ–Ω–∞"], test_list)

        # TODO:
        #    test_list = self.tokenizer.tokenize("–≤ 1996,1997,1998")
        #    self.assertEqual(["–≤", " ", "1996,1997,1998"], test_list)

        test_list = self.tokenizer.tokenize("2 000 —Ç–æ–Ω –∑ 12 000 –≤—ñ–¥–µ—Ä")
        self.assertEqual(["2 000", " ", "—Ç–æ–Ω", " ", "–∑", " ", "12 000", " ", "–≤—ñ–¥–µ—Ä"], test_list)

        test_list = self.tokenizer.tokenize("–Ω–∞–¥—ñ–π—à–ª–æ 12 000 000 —Ç–æ–Ω")
        self.assertEqual(["–Ω–∞–¥—ñ–π—à–ª–æ", " ", "12 000 000", " ", "—Ç–æ–Ω"], test_list)

        test_list = self.tokenizer.tokenize("–Ω–∞–¥—ñ–π—à–ª–æ 12\u202F000\u202F000 —Ç–æ–Ω")
        self.assertEqual(["–Ω–∞–¥—ñ–π—à–ª–æ", " ", "12 000 000", " ", "—Ç–æ–Ω"], test_list)

        test_list = self.tokenizer.tokenize("–¥–æ 01.01.42 400 000 —à—Ç.")
        self.assertEqual(["–¥–æ", " ", "01.01.42", " ", "400 000", " ", "—à—Ç."], test_list)

        # should not merge these numbers
        test_list = self.tokenizer.tokenize("2 15 –º—ñ–ª—å—è—Ä–¥—ñ–≤")
        self.assertEqual(["2", " ", "15", " ", "–º—ñ–ª—å—è—Ä–¥—ñ–≤"], test_list)

        test_list = self.tokenizer.tokenize("—É 2004 200 –º—ñ–ª—å—è—Ä–¥—ñ–≤")
        self.assertEqual(["—É", " ", "2004", " ", "200", " ", "–º—ñ–ª—å—è—Ä–¥—ñ–≤"], test_list)

        test_list = self.tokenizer.tokenize("–≤ –±—é–¥–∂–µ—Ç—ñ-2004 200 –º—ñ–ª—å—è—Ä–¥—ñ–≤")
        self.assertEqual(["–≤", " ", "–±—é–¥–∂–µ—Ç—ñ-2004", " ", "200", " ", "–º—ñ–ª—å—è—Ä–¥—ñ–≤"], test_list)

        test_list = self.tokenizer.tokenize("–∑ 12 0001 –≤—ñ–¥–µ—Ä")
        self.assertEqual(["–∑", " ", "12", " ", "0001", " ", "–≤—ñ–¥–µ—Ä"], test_list)

        test_list = self.tokenizer.tokenize("—Å—Ç–∞–ª–æ—Å—è 14.07.2001 –≤–Ω–æ—á—ñ")
        self.assertEqual(["—Å—Ç–∞–ª–æ—Å—è", " ", "14.07.2001", " ", "–≤–Ω–æ—á—ñ"], test_list)

        test_list = self.tokenizer.tokenize("–≤—á–æ—Ä–∞ –æ 7.30 —Ä–∞–Ω–∫—É")
        self.assertEqual(["–≤—á–æ—Ä–∞", " ", "–æ", " ", "7.30", " ", "—Ä–∞–Ω–∫—É"], test_list)

        test_list = self.tokenizer.tokenize("–≤—á–æ—Ä–∞ –æ 7:30 —Ä–∞–Ω–∫—É")
        self.assertEqual(["–≤—á–æ—Ä–∞", " ", "–æ", " ", "7:30", " ", "—Ä–∞–Ω–∫—É"], test_list)

        test_list = self.tokenizer.tokenize("3,5-5,6% 7¬∞ 7,4¬∞–°")
        self.assertEqual(["3,5-5,6", "%", " ", "7", "¬∞", " ", "7,4", "¬∞", "–°"], test_list)

        test_list = self.tokenizer.tokenize("–≤—ñ–¥–±—É–ª–∞—Å—è 17.8.1245")
        self.assertEqual(["–≤—ñ–¥–±—É–ª–∞—Å—è", " ", "17.8.1245"], test_list)

    def test_numbersmissingspace(self):
        test_list: list[str] = self.tokenizer.tokenize("–≤—ñ–¥ 12 –¥–æ14 —Ä–æ–∫—ñ–≤")
        self.assertEqual(["–≤—ñ–¥", " ", "12", " ", "–¥–æ", "14", " ", "—Ä–æ–∫—ñ–≤"], test_list)

        test_list = self.tokenizer.tokenize("–¥–æ14-15")
        self.assertEqual(["–¥–æ", "14-15"], test_list)

        test_list = self.tokenizer.tokenize("–¢.–®–µ–≤—á–µ–Ω–∫–∞53")
        self.assertEqual(["–¢.", "–®–µ–≤—á–µ–Ω–∫–∞", "53"], test_list)

        #    test_list = self.tokenizer.tokenize("¬´–¢–µ–Ω¬ª103.")
        #    self.assertEqual(["¬´", "–¢–µ–Ω", "¬ª", "103", "."], test_list)

        test_list = self.tokenizer.tokenize("¬´–ú–∞–∫2¬ª")
        self.assertEqual(["¬´", "–ú–∞–∫2", "¬ª"], test_list)

        test_list = self.tokenizer.tokenize("–∫–º2")
        self.assertEqual(["–∫–º2"], test_list)

        test_list = self.tokenizer.tokenize("000—Ö—Ö—Ö000")
        self.assertEqual(["000—Ö—Ö—Ö000"], test_list)

    def test_plus(self):
        test_list: list[str] = self.tokenizer.tokenize("+20")
        self.assertEqual(["+20"], test_list)

        test_list = self.tokenizer.tokenize("–ø—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫+–∑–∞–π–º–µ–Ω–Ω–∏–∫")
        self.assertEqual(["–ø—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫", "+", "–∑–∞–π–º–µ–Ω–Ω–∏–∫"], test_list)

        test_list = self.tokenizer.tokenize("+–∑–∞–π–º–µ–Ω–Ω–∏–∫")
        self.assertEqual(["+", "–∑–∞–π–º–µ–Ω–Ω–∏–∫"], test_list)

        test_list = self.tokenizer.tokenize("–†–æ—Ç—Ç–µ—Ä–¥–∞–º+ ")
        self.assertEqual(["–†–æ—Ç—Ç–µ—Ä–¥–∞–º+", " "], test_list)

    def test_tokenize(self):
        test_list: list[str] = self.tokenizer.tokenize("–í–æ–Ω–∏ –ø—Ä–∏–π—à–ª–∏ –¥–æ–¥–æ–º—É.")
        self.assertEqual(["–í–æ–Ω–∏", " ", "–ø—Ä–∏–π—à–ª–∏", " ", "–¥–æ–¥–æ–º—É", "."], test_list)

        test_list = self.tokenizer.tokenize("–í–æ–Ω–∏ –ø—Ä–∏–π—à–ª–∏ –ø º—è—Ç–∏–º–∏ –∑—ñ–≤‚Äô—è–ª–∏–º–∏.")
        self.assertEqual(["–í–æ–Ω–∏", " ", "–ø—Ä–∏–π—à–ª–∏", " ", "–ø'—è—Ç–∏–º–∏", " ", "–∑—ñ–≤'—è–ª–∏–º–∏", "."], test_list)

        #    test_list = self.tokenizer.tokenize("–í–æ–Ω–∏\u0301 –ø—Ä–∏\u00AD–π—à–ª–∏ –ø º—è\u0301—Ç–∏–º–∏ –∑—ñ–≤‚Äô—è\u00AD–ª–∏–º–∏.")
        #    self.assertEqual(["–í–æ–Ω–∏", " ", "–ø—Ä–∏–π—à–ª–∏", " ", "–ø'—è—Ç–∏–º–∏", " ", "–∑—ñ–≤'—è–ª–∏–º–∏", "."], test_list)

        test_list = self.tokenizer.tokenize("—è —É–∫—Ä–∞—ó–Ω–µ—Ü—å(—Å–º—ñ—î—Ç—å—Å—è")
        self.assertEqual(["—è", " ", "—É–∫—Ä–∞—ó–Ω–µ—Ü—å", "(", "—Å–º—ñ—î—Ç—å—Å—è"], test_list)

        test_list = self.tokenizer.tokenize("–û–£–ù(–±) —Ç–∞ –ö–ü(–±)–£")
        self.assertEqual(["–û–£–ù(–±)", " ", "—Ç–∞", " ", "–ö–ü(–±)–£"], test_list)

        test_list = self.tokenizer.tokenize("–ù–µ–≥–æ–¥–∞ —î... –∑–∞—Å—Ç—É–ø–Ω–∏–∫–æ–º")
        self.assertEqual(["–ù–µ–≥–æ–¥–∞", " ", "—î", "...", " ", "–∑–∞—Å—Ç—É–ø–Ω–∏–∫–æ–º"], test_list)

        test_list = self.tokenizer.tokenize("–ó–∞–ø–∞–≥—É–±–∏–ª–∏!.. —Ç–∞–∫–æ–∂")
        self.assertEqual(["–ó–∞–ø–∞–≥—É–±–∏–ª–∏", "!..", " ", "—Ç–∞–∫–æ–∂"], test_list)

        test_list = self.tokenizer.tokenize("–¶–µ–π –≥—Ä–∞—Ñ–∏–Ω.")
        self.assertEqual(["–¶–µ–π", " ", "–≥—Ä–∞—Ñ–∏–Ω", "."], test_list)

        test_list = self.tokenizer.tokenize("‚Äî –ì–º.")
        self.assertEqual(["‚Äî", " ", "–ì–º", "."], test_list)

        test_list = self.tokenizer.tokenize("—Å—Ç—ñ–Ω\u00AD–∫—É")
        self.assertEqual(["—Å—Ç—ñ–Ω\u00AD–∫—É"], test_list)

        test_list = self.tokenizer.tokenize("—Å—Ç—ñ–Ω\u00AD\n–∫—É")
        self.assertEqual(["—Å—Ç—ñ–Ω\u00AD\n–∫—É"], test_list)

        # TODO: re-enable
        test_list = self.tokenizer.tokenize('–ø"—è–Ω–∏–π')
        self.assertEqual(['–ø"—è–Ω–∏–π'], test_list)

        test_list = self.tokenizer.tokenize("–í–µ—Ä–µ—Ç–µ–Ω–∏—Ü—è**")
        self.assertEqual(["–í–µ—Ä–µ—Ç–µ–Ω–∏—Ü—è", "**"], test_list)

        test_list = self.tokenizer.tokenize("–º–æ–≤—ñ***,")
        self.assertEqual(["–º–æ–≤—ñ", "***", ","], test_list)

        test_list = self.tokenizer.tokenize("*–û—Ä–µ–Ω–±—É—Ä–≥")
        self.assertEqual(["*", "–û—Ä–µ–Ω–±—É—Ä–≥"], test_list)

        test_list = self.tokenizer.tokenize("‚ñ∂–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è")
        self.assertEqual(["‚ñ∂", "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è"], test_list)

        test_list = self.tokenizer.tokenize("—É—Å–º—ñ—à–∫—ÉüòÅ")
        self.assertEqual(["—É—Å–º—ñ—à–∫—É", "üòÅ"], test_list)

        test_list = self.tokenizer.tokenize("–∑*—è—Å—É–≤–∞–≤")
        self.assertEqual(["–∑*—è—Å—É–≤–∞–≤"], test_list)

    def test_initials(self):
        test_list: list[str] = self.tokenizer.tokenize("–ó–∞—Å—ñ–¥–∞–≤ –Ü.–Ñ—Ä–º–æ–ª—é–∫.")
        self.assertEqual(["–ó–∞—Å—ñ–¥–∞–≤", " ", "–Ü.", "–Ñ—Ä–º–æ–ª—é–∫", "."], test_list)

        test_list = self.tokenizer.tokenize("–ó–∞—Å—ñ–¥–∞–≤ –Ü.   –Ñ—Ä–º–æ–ª—é–∫.")
        self.assertEqual(["–ó–∞—Å—ñ–¥–∞–≤", " ", "–Ü.", " ", " ", " ", "–Ñ—Ä–º–æ–ª—é–∫", "."], test_list)

        test_list = self.tokenizer.tokenize("–ó–∞—Å—ñ–¥–∞–≤ –Ü. –ü. –Ñ—Ä–º–æ–ª—é–∫.")
        self.assertEqual(["–ó–∞—Å—ñ–¥–∞–≤", " ", "–Ü.", " ", "–ü.", " ", "–Ñ—Ä–º–æ–ª—é–∫", "."], test_list)

        test_list = self.tokenizer.tokenize("–ó–∞—Å—ñ–¥–∞–≤ –Ü.–ü.–Ñ—Ä–º–æ–ª—é–∫.")
        self.assertEqual(["–ó–∞—Å—ñ–¥–∞–≤", " ", "–Ü.", "–ü.", "–Ñ—Ä–º–æ–ª—é–∫", "."], test_list)

        test_list = self.tokenizer.tokenize("–Ü.\u00A0–Ñ—Ä–º–æ–ª—é–∫.")
        self.assertEqual(["–Ü.", "\u00A0", "–Ñ—Ä–º–æ–ª—é–∫", "."], test_list)

        test_list = self.tokenizer.tokenize("–ó–∞—Å—ñ–¥–∞–≤ –Ñ—Ä–º–æ–ª—é–∫ –Ü.")
        self.assertEqual(["–ó–∞—Å—ñ–¥–∞–≤", " ", "–Ñ—Ä–º–æ–ª—é–∫", " ", "–Ü."], test_list)

        test_list = self.tokenizer.tokenize("–ó–∞—Å—ñ–¥–∞–≤ –Ñ—Ä–º–æ–ª—é–∫ –Ü. –ü.")
        self.assertEqual(["–ó–∞—Å—ñ–¥–∞–≤", " ", "–Ñ—Ä–º–æ–ª—é–∫", " ", "–Ü.", " ", "–ü."], test_list)

        test_list = self.tokenizer.tokenize("–ó–∞—Å—ñ–¥–∞–≤ –Ñ—Ä–º–æ–ª—é–∫ –Ü. —Ç–∞ —ñ–Ω—à—ñ")
        self.assertEqual(["–ó–∞—Å—ñ–¥–∞–≤", " ", "–Ñ—Ä–º–æ–ª—é–∫", " ", "–Ü.", " ", "—Ç–∞", " ", "—ñ–Ω—à—ñ"], test_list)

    def test_abbreviations(self):
        # —Å–∫–æ—Ä–æ—á–µ–Ω–Ω—è
        test_list: list[str] = self.tokenizer.tokenize("140 —Ç–∏—Å. –ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫—ñ–≤")
        self.assertEqual(["140", " ", "—Ç–∏—Å.", " ", "–ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫—ñ–≤"], test_list)

        test_list = self.tokenizer.tokenize("450 —Ç–∏—Å. 297 –≥—Ä–Ω")
        self.assertEqual(["450", " ", "—Ç–∏—Å.", " ", "297", " ", "–≥—Ä–Ω"], test_list)

        test_list = self.tokenizer.tokenize("297 –≥—Ä–Ω...")
        self.assertEqual(["297", " ", "–≥—Ä–Ω", "..."], test_list)

        test_list = self.tokenizer.tokenize("297 –≥—Ä–Ω.")
        self.assertEqual(["297", " ", "–≥—Ä–Ω", "."], test_list)

        #    test_list = self.tokenizer.tokenize("297 –≥—Ä–Ω.!!!")
        #    self.assertEqual(["297", " ", "–≥—Ä–Ω.", "!!!"], test_list)

        #    test_list = self.tokenizer.tokenize("297 –≥—Ä–Ω.??")
        #    self.assertEqual(["297", " ", "–≥—Ä–Ω.", "??"], test_list)

        test_list = self.tokenizer.tokenize("450 —Ç–∏—Å.")
        self.assertEqual(["450", " ", "—Ç–∏—Å."], test_list)

        test_list = self.tokenizer.tokenize("450 —Ç–∏—Å.\n")
        self.assertEqual(["450", " ", "—Ç–∏—Å.", "\n"], test_list)

        test_list = self.tokenizer.tokenize("354\u202F—Ç–∏—Å.")
        self.assertEqual(["354", "\u202F", "—Ç–∏—Å."], test_list)

        test_list = self.tokenizer.tokenize("911 —Ç–∏—Å.–≥—Ä–Ω. –∑ –±—é–¥–∂–µ—Ç—É")
        self.assertEqual(["911", " ", "—Ç–∏—Å.", "–≥—Ä–Ω", ".", " ", "–∑", " ", "–±—é–¥–∂–µ—Ç—É"], test_list)

        # TODO: re-enable
        test_list = self.tokenizer.tokenize("–∑–∞ $400\n  —Ç–∏—Å., –∑–¥–∞–≤–∞–ª–æ—Å—è –±")
        self.assertEqual(["–∑–∞", " ", "$", "400", "\n", " ", " ", "—Ç–∏—Å.", ",", " ", "–∑–¥–∞–≤–∞–ª–æ—Å—è", " ", "–±"], test_list)

        test_list = self.tokenizer.tokenize("–Ω–∞–π–≤–∞–∂—á–æ–≥–æ –∂–∞–Ω—Ä—É‚Äî –æ–ø–æ–≤—ñ–¥–∞–Ω–Ω—è")
        self.assertEqual(["–Ω–∞–π–≤–∞–∂—á–æ–≥–æ", " ", "–∂–∞–Ω—Ä—É", "‚Äî", " ", "–æ–ø–æ–≤—ñ–¥–∞–Ω–Ω—è"], test_list)

        test_list = self.tokenizer.tokenize("–ø—Ä–æ—Ñ. –ê—Ä—Ç—é—Ö–æ–≤")
        self.assertEqual(["–ø—Ä–æ—Ñ.", " ", "–ê—Ä—Ç—é—Ö–æ–≤"], test_list)

        test_list = self.tokenizer.tokenize("–ø—Ä–æ—Ñ.\u00A0–ê—Ä—Ç—é—Ö–æ–≤")
        self.assertEqual(["–ø—Ä–æ—Ñ.", "\u00A0", "–ê—Ä—Ç—é—Ö–æ–≤"], test_list)

        test_list = self.tokenizer.tokenize("–Ü–≤. –§—Ä–∞–Ω–∫–æ")
        self.assertEqual(["–Ü–≤.", " ", "–§—Ä–∞–Ω–∫–æ"], test_list)

        test_list = self.tokenizer.tokenize("–∫—É—Ç—é\u00A0‚Äî —â–µ–¥—Ä—É")
        self.assertEqual(["–∫—É—Ç—é", "\u00A0", "‚Äî", " ", "—â–µ–¥—Ä—É"], test_list)

        test_list = self.tokenizer.tokenize("—Ç–∞–∫–æ–∂ –∑–∞–≤. –≤—ñ–¥–¥—ñ–ª–æ–º")
        self.assertEqual(["—Ç–∞–∫–æ–∂", " ", "–∑–∞–≤.", " ", "–≤—ñ–¥–¥—ñ–ª–æ–º"], test_list)

        test_list = self.tokenizer.tokenize("–¥–æ –Ω. –µ.")
        self.assertEqual(["–¥–æ", " ", "–Ω.", " ", "–µ."], test_list)

        test_list = self.tokenizer.tokenize("–¥–æ –Ω.–µ.")
        self.assertEqual(["–¥–æ", " ", "–Ω.", "–µ."], test_list)

        test_list = self.tokenizer.tokenize("–≤. –æ. –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞")
        self.assertEqual(["–≤.", " ", "–æ.", " ", "–Ω–∞—á–∞–ª—å–Ω–∏–∫–∞"], test_list)

        test_list = self.tokenizer.tokenize("–≤.–æ. –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞")
        self.assertEqual(["–≤.", "–æ.", " ", "–Ω–∞—á–∞–ª—å–Ω–∏–∫–∞"], test_list)

        test_list = self.tokenizer.tokenize("100 –∫.—Å.")
        self.assertEqual(["100", " ", "–∫.", "—Å."], test_list)

        test_list = self.tokenizer.tokenize("1998 —Ä.–Ω.")
        self.assertEqual(["1998", " ", "—Ä.", "–Ω."], test_list)

        test_list = self.tokenizer.tokenize("22 –∫–æ–ø.")
        self.assertEqual(["22", " ", "–∫–æ–ø."], test_list)

        test_list = self.tokenizer.tokenize("800 –≥—Ä. –º'—è—Å–∞")
        self.assertEqual(["800", " ", "–≥—Ä.", " ", "–º'—è—Å–∞"], test_list)

        test_list = self.tokenizer.tokenize("18-19 —Å—Ç.—Å—Ç. –±—É–ª–∏")
        self.assertEqual(["18-19", " ", "—Å—Ç.", "—Å—Ç.", " ", "–±—É–ª–∏"], test_list)

        test_list = self.tokenizer.tokenize("–Ü —Å—Ç. 11")
        self.assertEqual(["–Ü", " ", "—Å—Ç.", " ", "11"], test_list)

        test_list = self.tokenizer.tokenize("–∫—É–±. –º")
        self.assertEqual(["–∫—É–±.", " ", "–º"], test_list)

        test_list = self.tokenizer.tokenize("–∫—É–±.–º")
        self.assertEqual(["–∫—É–±.", "–º"], test_list)

        test_list = self.tokenizer.tokenize("–£ —Å. –í–∏–∂–≤–∞")
        self.assertEqual(["–£", " ", "—Å.", " ", "–í–∏–∂–≤–∞"], test_list)

        test_list = self.tokenizer.tokenize("–î–æ–≤–∂–∏–Ω–æ—é 30 —Å–º. –∑ –≥–∞–∫–æ–º.")
        self.assertEqual(["–î–æ–≤–∂–∏–Ω–æ—é", " ", "30", " ", "—Å–º", ".", " ", "–∑", " ", "–≥–∞–∫–æ–º", "."], test_list)

        test_list = self.tokenizer.tokenize("–î–æ–≤–∂–∏–Ω–æ—é 30 —Å–º. –ü–æ—ó—Ö–∞–ª–∏.")
        self.assertEqual(["–î–æ–≤–∂–∏–Ω–æ—é", " ", "30", " ", "—Å–º", ".", " ", "–ü–æ—ó—Ö–∞–ª–∏", "."], test_list)

        test_list = self.tokenizer.tokenize("100 –º. –¥–æ—Ä–æ–≥–∏.")
        self.assertEqual(["100", " ", "–º", ".", " ", "–¥–æ—Ä–æ–≥–∏", "."], test_list)

        test_list = self.tokenizer.tokenize("–≤ –º.–ö–∏—ó–≤")
        self.assertEqual(["–≤", " ", "–º.", "–ö–∏—ó–≤"], test_list)

        test_list = self.tokenizer.tokenize("–ù–∞ –≤–∏—Å–æ—Ç—ñ 4000 –º...")
        self.assertEqual(["–ù–∞", " ", "–≤–∏—Å–æ—Ç—ñ", " ", "4000", " ", "–º", "..."], test_list)

        test_list = self.tokenizer.tokenize("‚Ññ47 (–º. –°–ª–æ–≤'—è–Ω—Å—å–∫)")
        self.assertEqual(["‚Ññ47", " ", "(", "–º.", " ", "–°–ª–æ–≤'—è–Ω—Å—å–∫", ")"], test_list)

        test_list = self.tokenizer.tokenize("—Å.-–≥.")
        self.assertEqual(["—Å.-–≥."], test_list)

        test_list = self.tokenizer.tokenize("100 –≥—Ä–Ω. –≤ –±–∞–Ω–∫")
        self.assertEqual(["100", " ", "–≥—Ä–Ω", ".", " ", "–≤", " ", "–±–∞–Ω–∫"], test_list)

        test_list = self.tokenizer.tokenize("—Ç–∞–∫–µ —Ç–∞ —ñ–Ω.")
        self.assertEqual(["—Ç–∞–∫–µ", " ", "—Ç–∞", " ", "—ñ–Ω."], test_list)

        test_list = self.tokenizer.tokenize("—ñ —Ç. —ñ–Ω.")
        self.assertEqual(["—ñ", " ", "—Ç.", " ", "—ñ–Ω."], test_list)

        test_list = self.tokenizer.tokenize("—ñ —Ç.–¥.")
        self.assertEqual(["—ñ", " ", "—Ç.", "–¥."], test_list)

        test_list = self.tokenizer.tokenize("–≤ —Ç. —á.")
        self.assertEqual(["–≤", " ", "—Ç.", " ", "—á."], test_list)

        test_list = self.tokenizer.tokenize("–¥–æ —Ç. –∑–≤. —Å–∞–ª—å–æ–Ω—É")
        self.assertEqual(["–¥–æ", " ", "—Ç.", " ", "–∑–≤.", " ", "—Å–∞–ª—å–æ–Ω—É"], test_list)

        test_list = self.tokenizer.tokenize(" —ñ –ø–æ–¥.")
        self.assertEqual([" ", "—ñ", " ", "–ø–æ–¥."], test_list)

        test_list = self.tokenizer.tokenize("–Ü–Ω—Å—Ç–∏—Ç—É—Ç —ñ–º. –∞–∫–∞–¥. –í–µ—Ä–Ω–∞–¥—Å—å–∫–æ–≥–æ.")
        self.assertEqual(["–Ü–Ω—Å—Ç–∏—Ç—É—Ç", " ", "—ñ–º.", " ", "–∞–∫–∞–¥.", " ", "–í–µ—Ä–Ω–∞–¥—Å—å–∫–æ–≥–æ", "."], test_list)

        test_list = self.tokenizer.tokenize("–ü–∞–ª–∞—Ü —ñ–º. –≥–µ—Ç—å–º–∞–Ω–∞ –°–∫–æ—Ä–æ–ø–∞–¥—Å—å–∫–æ–≥–æ.")
        self.assertEqual(["–ü–∞–ª–∞—Ü", " ", "—ñ–º.", " ", "–≥–µ—Ç—å–º–∞–Ω–∞", " ", "–°–∫–æ—Ä–æ–ø–∞–¥—Å—å–∫–æ–≥–æ", "."], test_list)

        test_list = self.tokenizer.tokenize("–≤—ñ–¥ –ª–∞—Ç. momento")
        self.assertEqual(["–≤—ñ–¥", " ", "–ª–∞—Ç.", " ", "momento"], test_list)

        test_list = self.tokenizer.tokenize("–Ω–∞ 1-–∫—ñ–º–Ω. –∫–≤. –≤ —Ü–µ–Ω—Ç—Ä—ñ")
        self.assertEqual(["–Ω–∞", " ", "1-–∫—ñ–º–Ω.", " ", "–∫–≤.", " ", "–≤", " ", "—Ü–µ–Ω—Ç—Ä—ñ"], test_list)

        test_list = self.tokenizer.tokenize("1 –∫–≤. –∫–º.")
        self.assertEqual(["1", " ", "–∫–≤.", " ", "–∫–º", "."], test_list)

        test_list = self.tokenizer.tokenize("–í–∞–ª–µ—Ä—ñ–π (–º—ñ–ª—ñ—Ü—ñ–æ–Ω–µ—Ä-–ø–∞—Ä–æ–¥–∏—Å—Ç.\n‚Äì  –ê–≤—Ç.) —Å—Ç–∞–Ω–µ –ø–∞—Ä–æ–¥–∏—Å—Ç–æ–º.")
        self.assertEqual(
            [
                "–í–∞–ª–µ—Ä—ñ–π",
                " ",
                "(",
                "–º—ñ–ª—ñ—Ü—ñ–æ–Ω–µ—Ä-–ø–∞—Ä–æ–¥–∏—Å—Ç",
                ".",
                "\n",
                "‚Äì",
                " ",
                " ",
                "–ê–≤—Ç.",
                ")",
                " ",
                "—Å—Ç–∞–Ω–µ",
                " ",
                "–ø–∞—Ä–æ–¥–∏—Å—Ç–æ–º",
                ".",
            ],
            test_list,
        )

        test_list = self.tokenizer.tokenize("–°—å–æ–≥–æ–¥–Ω—ñ (—É —á–µ—Ç–≤–µ—Ä.  ‚Äî –†–µ–¥.), –≤—Ä–∞–Ω—Ü—ñ.")
        self.assertEqual(
            ["–°—å–æ–≥–æ–¥–Ω—ñ", " ", "(", "—É", " ", "—á–µ—Ç–≤–µ—Ä", ".", " ", " ", "‚Äî", " ", "–†–µ–¥.", ")", ",", " ", "–≤—Ä–∞–Ω—Ü—ñ", "."],
            test_list,
        )

        test_list = self.tokenizer.tokenize(
            "Fair trade [¬´–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞ —Ç–æ—Ä–≥—ñ–≤–ª—è¬ª. ‚Äì    –ê–≤—Ç.], —è–∫–∏–π —Å—Ç–µ–∂–∏—Ç—å –∑–∞ —Ç–∏–º, —â–æ–± —É –∫—Ä–∞—ó–Ω–∞—Ö"
        )
        self.assertTrue("–ê–≤—Ç." in test_list)

        test_list = self.tokenizer.tokenize("–¥–∏–≤–æ –∑ –¥–∏–≤.")
        self.assertEqual(["–¥–∏–≤–æ", " ", "–∑", " ", "–¥–∏–≤", "."], test_list)

        test_list = self.tokenizer.tokenize("–¥–∏–≤–æ –∑ –¥–∏–≤...")
        self.assertEqual(["–¥–∏–≤–æ", " ", "–∑", " ", "–¥–∏–≤", "..."], test_list)

        test_list = self.tokenizer.tokenize("—Ç–µ–ª.: 044-425-20-63")
        self.assertEqual(["—Ç–µ–ª.", ":", " ", "044-425-20-63"], test_list)

        test_list = self.tokenizer.tokenize("—Å/–≥")
        self.assertEqual(["—Å/–≥"], test_list)

        test_list = self.tokenizer.tokenize("—ñ–º.–í–∞—Å–∏–ª—è")
        self.assertEqual(["—ñ–º.", "–í–∞—Å–∏–ª—è"], test_list)

        test_list = self.tokenizer.tokenize("—Å—Ç.231")
        self.assertEqual(["—Å—Ç.", "231"], test_list)

        test_list = self.tokenizer.tokenize("2016-2017—Ä—Ä.")
        self.assertEqual(["2016-2017", "—Ä—Ä."], test_list)

        test_list = self.tokenizer.tokenize("30.04.2010—Ä.")
        self.assertEqual(["30.04.2010", "—Ä."], test_list)

        test_list = self.tokenizer.tokenize("–Ω—ñ –º–æ–≥–∏–ª–∏ 6–≤. ")
        self.assertEqual(["–Ω—ñ", " ", "–º–æ–≥–∏–ª–∏", " ", "6–≤", ".", " "], test_list)

        test_list = self.tokenizer.tokenize("–≤... –æ–¥—è–≥–Ω–µ–Ω–æ–º—É")
        self.assertEqual(["–≤", "...", " ", "–æ–¥—è–≥–Ω–µ–Ω–æ–º—É"], test_list)

        # invaild but happens
        test_list = self.tokenizer.tokenize("10 –º–ª–Ω. —á–æ–ª–æ–≤—ñ–∫")
        self.assertEqual(["10", " ", "–º–ª–Ω.", " ", "—á–æ–ª–æ–≤—ñ–∫"], test_list)

        test_list = self.tokenizer.tokenize("–≤—ñ–¥ –¢–∞–≤—Ä—ñ–π—Å—å–∫–æ—ó –≥—É–±.5")
        self.assertEqual(["–≤—ñ–¥", " ", "–¢–∞–≤—Ä—ñ–π—Å—å–∫–æ—ó", " ", "–≥—É–±.", "5"], test_list)

        test_list = self.tokenizer.tokenize("–≤—ñ–¥ —á–µ—Ä–≤–æ–Ω–∏—Ö –≥—É–±.")
        self.assertEqual(["–≤—ñ–¥", " ", "—á–µ—Ä–≤–æ–Ω–∏—Ö", " ", "–≥—É–±", "."], test_list)

        test_list = self.tokenizer.tokenize("–ö.-–°–≤—è—Ç–æ—à–∏–Ω—Å—å–∫–∏–π")
        self.assertEqual(["–ö.-–°–≤—è—Ç–æ—à–∏–Ω—Å—å–∫–∏–π"], test_list)

        test_list = self.tokenizer.tokenize("–ö.-–ì. –†—É—Ñ—Ñ–º–∞–Ω")
        self.assertEqual(["–ö.-–ì.", " ", "–†—É—Ñ—Ñ–º–∞–Ω"], test_list)

        test_list = self.tokenizer.tokenize("–†–∏—Å. 10")
        self.assertEqual(["–†–∏—Å.", " ", "10"], test_list)

        test_list = self.tokenizer.tokenize("—Ö—É–¥. —Ñ—ñ–ª—å–º")
        self.assertEqual(["—Ö—É–¥.", " ", "—Ñ—ñ–ª—å–º"], test_list)

        # not too frequent

    #    test_list = self.tokenizer.tokenize("30.04.10—Ä.")
    #    self.assertEqual(["30.04.10", "—Ä."], test_list)

    def test_brackets(self):
        # —Å–∫–æ—Ä–æ—á–µ–Ω–Ω—è
        test_list: list[str] = self.tokenizer.tokenize("–¥[–æ–∫—Ç–æ]—Ä[–æ–º]")
        self.assertEqual(["–¥[–æ–∫—Ç–æ]—Ä[–æ–º]"], test_list)

    def test_apostrophe(self):
        test_list: list[str] = self.tokenizer.tokenize("‚Äô–ø—Ä–æ–¥—É–∫—Ç–∏ —Ö–∞—Ä—á—É–≤–∞–Ω–Ω—è‚Äô")
        self.assertEqual(["'", "–ø—Ä–æ–¥—É–∫—Ç–∏", " ", "—Ö–∞—Ä—á—É–≤–∞–Ω–Ω—è", "'"], test_list)

        test_list = self.tokenizer.tokenize("—Å—Ö–µ–º–∞ '–≥—Ä–æ—à—ñ'")
        self.assertEqual(["—Å—Ö–µ–º–∞", " ", "'", "–≥—Ä–æ—à—ñ", "'"], test_list)

        test_list = self.tokenizer.tokenize("(‚Äò–¥–∑–µ—Ä–∫–∞–ª–æ‚Äô)")
        self.assertEqual(["(", "'", "–¥–∑–µ—Ä–∫–∞–ª–æ", "'", ")"], test_list)

        test_list = self.tokenizer.tokenize("–≤—Å–µ '–¥–Ω–æ –ø—ñ–¥—É")
        self.assertEqual(["–≤—Å–µ", " ", "'–¥–Ω–æ", " ", "–ø—ñ–¥—É"], test_list)

        test_list = self.tokenizer.tokenize("—Ç—Ä–æ—Ö–∏ '–¥–Ω–æ '–¥–Ω–æ–º—É —Å–∫–∞–∑–∞–Ω–æ")
        self.assertEqual(["—Ç—Ä–æ—Ö–∏", " ", "'–¥–Ω–æ", " ", "'–¥–Ω–æ–º—É", " ", "—Å–∫–∞–∑–∞–Ω–æ"], test_list)

        test_list = self.tokenizer.tokenize("–∞ –º–æ',")
        self.assertEqual(["–∞", " ", "–º–æ'", ","], test_list)

        test_list = self.tokenizer.tokenize("–ø—ñ–¥–µ–º–æ'")
        self.assertEqual(["–ø—ñ–¥–µ–º–æ", "'"], test_list)

        test_list = self.tokenizer.tokenize("–ó–î–û–†–û–í‚Äô–Ø.")
        self.assertEqual(["–ó–î–û–†–û–í'–Ø", "."], test_list)

        test_list = self.tokenizer.tokenize("''—É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π''")
        self.assertEqual(["''", "—É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π", "''"], test_list)

        # '—Ç—Å–µ, '–¥–¥–∞—Ç–∏  '–≥–æ

        test_list = self.tokenizer.tokenize("'—î")
        self.assertEqual(["'", "—î"], test_list)

        test_list = self.tokenizer.tokenize("'(—î)")
        self.assertEqual(["'", "(", "—î", ")"], test_list)

    def test_dash(self):
        test_list: list[str] = self.tokenizer.tokenize("–ö–∞–Ω‚Äô-–ö–∞ –ù–æ –†–µ–π")
        self.assertEqual(["–ö–∞–Ω'-–ö–∞", " ", "–ù–æ", " ", "–†–µ–π"], test_list)

        test_list = self.tokenizer.tokenize("—ñ –µ–∫—Å-¬´–¥–µ–ø—É—Ç–∞—Ç¬ª –≤–∏–±—É–≤")
        self.assertEqual(["—ñ", " ", "–µ–∫—Å-¬´–¥–µ–ø—É—Ç–∞—Ç¬ª", " ", "–≤–∏–±—É–≤"], test_list)

        test_list = self.tokenizer.tokenize('—Ç–∏—Ö "200"-—Ö –±–∞–≥–∞—Ç–æ')
        self.assertEqual(["—Ç–∏—Ö", " ", '"200"-—Ö', " ", "–±–∞–≥–∞—Ç–æ"], test_list)

        test_list = self.tokenizer.tokenize("¬´–¥—ñ–¥–∏¬ª-—É–∫—Ä–∞—ó–Ω—Ü—ñ")
        self.assertEqual(["¬´–¥—ñ–¥–∏¬ª-—É–∫—Ä–∞—ó–Ω—Ü—ñ"], test_list)

        #    test_list = self.tokenizer.tokenize("¬´–∫—Ä–∞–±¬ª-–ø–µ—Ä–µ—Ä–æ—Å—Ç–æ–∫")
        #    self.assertEqual(["¬´", "–∫—Ä–∞–±", "¬ª", "-", "–ø–µ—Ä–µ—Ä–æ—Å—Ç–æ–∫"], test_list)

        test_list = self.tokenizer.tokenize("–≤–µ—Ä–µ—Å–Ω—ñ--–∂–æ–≤—Ç–Ω—ñ")
        self.assertEqual(["–≤–µ—Ä–µ—Å–Ω—ñ", "--", "–∂–æ–≤—Ç–Ω—ñ"], test_list)

        test_list = self.tokenizer.tokenize("‚Äî–£ –ø–µ–≤–Ω–æ–º—É")
        self.assertEqual(["‚Äî", "–£", " ", "–ø–µ–≤–Ω–æ–º—É"], test_list)

        test_list = self.tokenizer.tokenize("-–£ –ø–µ–≤–Ω–æ–º—É")
        self.assertEqual(["-", "–£", " ", "–ø–µ–≤–Ω–æ–º—É"], test_list)

        test_list = self.tokenizer.tokenize("–ø—Ä–∞—Ü—è‚Äî–≥–æ–ª–æ–≤–∞")
        self.assertEqual(["–ø—Ä–∞—Ü—è", "‚Äî", "–≥–æ–ª–æ–≤–∞"], test_list)

        test_list = self.tokenizer.tokenize("–õ—é–¥–∏–Ω–∞‚Äî")
        self.assertEqual(["–õ—é–¥–∏–Ω–∞", "‚Äî"], test_list)

        test_list = self.tokenizer.tokenize("–•‚Äì–•–Ü")
        self.assertEqual(["–•", "‚Äì", "–•–Ü"], test_list)

        test_list = self.tokenizer.tokenize("VII-VIII")
        self.assertEqual(["VII", "-", "VIII"], test_list)

        test_list = self.tokenizer.tokenize("–°—Ç—Ä–∏–π‚Äì ")
        self.assertEqual(["–°—Ç—Ä–∏–π", "‚Äì", " "], test_list)

        test_list = self.tokenizer.tokenize("—Ñ—ñ—Ç–æ‚Äì —Ç–∞ —Ç–µ—Ä–º–æ—Ç–µ—Ä–∞–ø—ñ—ó")
        self.assertEqual(["—Ñ—ñ—Ç–æ‚Äì", " ", "—Ç–∞", " ", "—Ç–µ—Ä–º–æ—Ç–µ—Ä–∞–ø—ñ—ó"], test_list)

        test_list = self.tokenizer.tokenize(" ‚Äì–í–∏–¥—ñ–ª–µ–Ω–æ")
        self.assertEqual([" ", "‚Äì", "–í–∏–¥—ñ–ª–µ–Ω–æ"], test_list)

        test_list = self.tokenizer.tokenize("—Ç–∞–∫,\u2013—Ç–∞–∫")
        self.assertEqual(["—Ç–∞–∫", ",", "\u2013", "—Ç–∞–∫"], test_list)

    # def test_specialchars(self):
    #     text:str = "–†–ï–ê–õ–Ü–ó–ê–¶–Ü–á \u00AD\n" + "–°–Ü–õ–¨–°–¨–ö–û–ì–û–°–ü–û–î–ê–†–°–¨–ö–û–á"

    #     test_list: list[str] = self.tokenizer.tokenize(text).stream()
    #         .map(s -> s.replace("\n", "\\n").replace("\u00AD", "\\xAD"))
    #         .collect(Collectors.toList())
    #     self.assertEqual(["–†–ï–ê–õ–Ü–ó–ê–¶–Ü–á", " ", "\\xAD", "\\n", "–°–Ü–õ–¨–°–¨–ö–û–ì–û–°–ü–û–î–ê–†–°–¨–ö–û–á"], test_list)

    #     test_list = self.tokenizer.tokenize("–∞%–π–æ–≥–æ")
    #     self.assertEqual(["–∞", "%", "–π–æ–≥–æ"], test_list)

    #     test_list = self.tokenizer.tokenize("5%-–≥–æ")
    #     self.assertEqual(["5%-–≥–æ"], test_list)
